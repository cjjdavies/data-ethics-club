# Data Ethics Club: [UK National AI Strategy: Pillar 3 - Governing AI Effectively](https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version#pillar-3-governing-ai-effectively)

```{admonition} What's this? 
This is summary of Wednesday 3rd November's Data Ethics Club discussion, where we spoke about the document [UK National AI Strategy: Pillar 3 - Governing AI Effectively](https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version#pillar-3-governing-ai-effectively).

The summary was written by Huw Day, who tried to synthesise everyone's contributions to this document and the discussion. "We" = "someone at Data Ethics Club". 
Nina Di Cara and Natalie Thurlby helped with a final edit.
```

## Introduction

This discussion centred around the [UK National AI Strategy: Pillar 3 - Governing AI Effectively](https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version#pillar-3-governing-ai-effectively). As the piece notes, the main goals outlined in the piece are:

"Ensuring that national governance of AI technologies encourages innovation, investment, protects the public and safeguards our fundamental values, while working with global partners to promote the responsible development of AI internationally."

We discussed the merits and shortfalls of the various goals as well as the potential conflicts between them, in particular considering how to balance encouraging innovation whilst maintaining a strong ethical framework (which some would argue, we currently do not have in the AI sector).

## Which proposals were you most pleased to see from the report? Are there any that you do not think will be welcomed?

Our discussion bought us to current ethical standards for AI and how they might have to change. They may no longer be valid. A lot of effort needs to be put into innovating our ethics, as we innovate our technology. Perhaps the biggest issue of ethics in science today is that the scientific innovation is innevitably done before the ethical guidelines are introduced (and often before those guidelines are even considered). We have to constantly innovate and question our ethical frameworks to ensure best practises which adhere to our values. The report did a good job of raising issues of fairness, accountability, and bias in AI systems.

We thought it was good that the authors acknowledged that they cannot regulate AI in the same way as past regulatory frameworks. It is not viable to just put blanket legislation in the same way. We face a nuanced problem which requires nuanced solutions. A sector-specific approach seems very reasonable.

Discussing what a suitable regulatory body would look like led to us considering the dichotomy between accountability and efficiency. If the ethics board was managed by a single person, that person is extremely susceptibile to letting their bias affects outcomes. The way to get around this is to have ethics committees with a variety of viewpoints. Burearcracy inevitably slows down the government massively and this can get in the way of innovation. 

Perhaps a workaround is open source committees managed by trusted communities who can respond faster, but the UK government (and indeed, most governments) would likely be reluctant to give up their governing responsibilities to such an extent. 

Another solution we discussed was the ombudsman approach to regulation - a figure is given wide authority in regulatory decision-making, essentially asked to exercise judgement within a broad set of parameters. Typically they are drawn from industry, or at least have some deep understanding of the industry, so that their judgements can be well-founded. The concept originated in Sweden in 1809, with ombudsman coming from the Swedish for "legal representative". 

It has been adopted it in some UK markets ("ombudsman" often used synonymously with "Parliamentary Commissioner for Administration") but not always successfully. When you have an industry that is moving so quickly it is difficult (if not impossible) to have a regulatory system that can cope if you try to write down lots of rules. An ombudsman approach allows for greater speed of reaction and flexibility.

(As a quick note, ombudsman is used a gender neutral term, as the suffix "-man" is a gender neutral suffix in the Swedish language. Regardless, the term has fallen out of usage in recent years.)

How will the UK approach this problem across different sectors? Would there be a national/ethical committee for every industry? It sounds to us like a lot of hoops to jump through before we reach a solution.

We saw issue with a seeming dispraity between public and private sector regulation. Whilst the report outlined the public sector restrictions, it didn't seem like there would be any regulations in the private sector which is troubling but frankly not surprising.

What do the authors mean by innovation? Some of us feared a barrage of "new technologies" which will simply be a collection of linear regression soups garnished with a few "for" and "if" loops?

## How do you see regulation as potentially impacting your work? If you are not UK based, are there similarities/differences with any of your national regulations?

Is the European approach better? Does it give us more confidence? They have guidelines from the EU.
 
As we mentioned before with bureaucracy, the ethical processes can be really messy when lots of people get involved. What makes things trustworthy to who? Who should be involved in these decision makers? Is it better that the public are more soothed by the decision (populist, religious figures for example) or if there is a stronger technical and ethical base for the decision making? How do we decide what makes a strong "ethical base"? 

Should we seek to reassure the public with the decision making? We discussed the importance of diversity in decision making; different research backgrounds, different races, genders, sexuality etc. The more different experiences people have, the more likely we are to find an issue. We decided that being more likely to find ethical issues is a good thing, at the very least for ethical guidelines, if not necersarily for innovation. Of course if we wanted everyone to be happy we would get never get anything done (as noted last time in our discussion on Decolonising Academia). Even the best decision makers might not be trusted by the public. But are the general public the best people to decide what being trustworthy means?

Having different use-cases for different sectors allows flexibility in decision making. Rigidity in guidelines is important for drawing lines we should refuse to cross but needs to be adaptive as our technological capabilities and societal standards shift.

It was good that this report acknowledged fairness bias, accountability and the commission on race and ethnic disparity. However, it wasn't clear the report was sure on the practicalities of what they wanted - do they want new laws and legislation? 

There were concerns about removing the need for government departments to tell people how algorithms make decisions in the name of innovation. For example, if access to universal credit was decided by an algorithm then there would be deep concerns about the right for you to know why you do/do not get that access. If you cannot understand how something works, you cannot question it effectively. This is a common theme explored in Data Ethics books such as [Weapons of Math Destruction by Cathy O'Neil](https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction) and [Automating Inequality by Virginia Eubanks](https://blogs.lse.ac.uk/lsereviewofbooks/2018/07/02/book-review-automating-inequality-how-high-tech-tools-profile-police-and-punish-the-poor-by-virginia-eubanks/). 

The next steps the report outlines seem somewhat vague; write more reports, remove, change and add some things but without clear direction etc. Perhaps in a rapidly developing field, it would feel premature to outline in detail things that are likely to change, but perhaps this vagueness should be accompanied with an admission of uncertainty. Nobody knows what the future of AI will look like so any legislation looking to direct it will necessarily need to be adaptive.

Whilst it was good that the strategy involved upskilling people, there was a clear emphasis of getting rid of all the EU Laws we had been following and reform the [GDPR](https://gdpr-info.eu/).

Below are some interesting links for the European Union guidelines for AI and some specific information about Italian AI Strategy:
- [Ethics guidelines for trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai) {revised 2021}.
- [Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS](https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206).

The first document tries to explain AI and current state-of-the-art AI systems. The EU proposals document define a set of rules to regulate AI systems and their possible misuses: 
"Title  II  establishes  a  list  of  prohibited  AI" "Title III contains specific rules for AI systems that create a high risk to the health and safety or  fundamental  rights  of  natural  persons." "Title IV concerns certain AI systems to take account of the specific risks of manipulation they pose."

- [Italy AI Strategy Report](https://knowledge4policy.ec.europa.eu/ai-watch/italy-ai-strategy-report_en) (2021).
- [Policy initiatives for Italy](https://oecd.ai/en/dashboards/policy-initiatives?conceptUris=http:%2F%2Fkim.oecd.org%2FTaxonomy%2FGeographicalAreas%23Italy).
- [Italian AI white paper 2018](https://ia.italia.it/assets/whitepaper.pdf).

In 2018 Italy adopted a national strategy on AI, following EU guidelines and the example of other European countries. The Italian government will release a new plan in 2022.

- [Strategia nazionale per l'intelligenza artificiale - Contesto](https://www.mise.gov.it/index.php/it/strategia-intelligenza-artificiale/contesto) (in Italian).
- [Draghi’s strategy for Italy’s AI - decode39](https://decode39.com/2162/italy-ai-strategy-draghi/).

"Operating within the framework of the coordinated plan on European AI published in December 2018, Mr Draghi’s government strategy aims to increase public funds for AI research".

## The report states that they want to build the “most trusted and pro-innovation system for AI governance in the world” - can both these things be true?

Ideally you want to find as many bugs as you can. You cannot find all of them, as you have limited time. So at what point do you start just calling them features? What sort of prescedent does that set? Could you balance reducing the bureacracy and innovative drive? Probably not consistently. But if you consider the pharamceutical industry's approach to developing various COVID 19 vaccines, private companies were able to maintain the strict standards of their respective ethical bodies but with a streamlined process (in their case, producing batches for the next stage of the trial before they had moved along to that trial, risking wasting that entire batch should it fail the stage of the trial).

We recalled studies where different people were given a software task. They found that diverse teams reduce errors because people with different backgrounds made different errors. 

You will not have innovation in the long term if people do not trust your product - they will just pull the plug. A lot of the AI being commerical ties into what public thinks about it. If people don't like a product, they won't use it. Public opinion almost forms an accidental ethics board in a more consumerist, capitalist society. 

Consumerism should not be the only line of defense to unethical practise. We need formal ethical boards to block unethical developments as well as guide innovators. Scientists should innovate in tramlines rather than in an open space.

Facebook appear to have reached a point of innovating too hard and not building in enough data protections where we need. In order for people to trust that you're going to innovate things, you need to show you're going to say no sometimes and that there is a line in the sand. Journals seem to not have as many moral standards and are more focussed on "quality" of science and innovation. They also need to do better.

Google had an Ethical AI team, but [they got fired](https://www.bbc.co.uk/news/technology-56135817) when they spoke up about unethical practises. We cannot rely on tech companies to regulate themsleves as we have seen previously in discussions on 

There aren't typically headlines about ethics reviews being rejected. Often there aren't reviews to pass through, but a journal shouldn't tweet everytime they reject a paper. But perhaps highlighting "hey this kind of paper is not entirely ethical" would be a worthwhile. These usually take the form of twitter feeds expressing outrage at certain developments, but not formal objections. 

There is an interesting analogy with pharmaceuticals and drug testing. There are stringent requirements in the pharmaceutical industries enforced by government agencies such as the [MHRA](https://www.gov.uk/government/organisations/medicines-and-healthcare-products-regulatory-agency) in the UK and the [FDA](https://www.fda.gov/) in the US. Can we get something like this for AI?

What would the phases of AI trials look like? It would probably depend on what sort of system you were testing, just as there are different [clinical trials](https://en.wikipedia.org/wiki/Clinical_trial#Trials_of_drugs) for drugs, procedures and devices. the system (unlike drugs) throughout that process. Would it involve some sort of [Randomised Controlled Trial](https://en.wikipedia.org/wiki/Randomized_controlled_trial)?

Whilst this is a nice idea in theory, private companies run the game with AI and it's unlikely a government institution will be able to dislodge half of Apple or Google, especially against the uphill battle of lobbying. This is the tradeoff of the consumerism focus of society. Consumers have a small sway in the directions these companies go by favouring certain products, but the more we support and consume AI developments, the more purview we give these tech giants.

--- 

## Attendees

__Name, Role, Affiliation, Where to find you, Emoji to describe your day__
- Natalie Thurlby, Data Scientist, University of Bristol, [NatalieThurlby](https://github.com/NatalieThurlby/), [@StatalieT](https://twitter.com/StatalieT) 
- Nina Di Cara, PhD Student, University of Bristol, [ninadicara](https://github.com/ninadicara/), [@ninadicara](https://twitter.com/ninadicara)
- Huw Day, PhDoer, University of Bristol, [@disco_huw](https://twitter.com/disco_huw)
- Euan Bennet, Senior Research Associate, University of Bristol, [@DrEuanBennet](https://twitter.com/DrEuanBennet)
- Angelo Varlotta, [@varl42](twitter.com/varl42)
- Laura Sheppard, PhD student, CASA - UCL, [@laurahsheppard](https://twitter.com/laurahsheppard)
- Roman Shkunov, maths/CS student, University of Bristol, [@RShkunov](https://twitter.com/RShkunov)
