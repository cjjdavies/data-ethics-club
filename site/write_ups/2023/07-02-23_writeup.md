# Data Ethics Club: [ChatGPT listed as an author on research papers: many scientists disapprove](https://www.nature.com/articles/d41586-023-00107-z)

```{admonition} What's this? 
This is summary of Wednesday 7th February's Data Ethics Club discussion, where we spoke [ChatGPT listed as an author on research papers: many scientists disapprove](https://www.nature.com/articles/d41586-023-00107-z), a Nature article written by Chris Stokel-Walker. 
The summary was written by Huw Day, who tried to synthesise everyone's contributions to this document and the discussion. "We" = "someone at Data Ethics Club". 
Nina Di Cara and Natalie Thurlby helped with the final edit.
```

## What are our experiences with Chat GPT?

Many of us haven't used it but it's obviously quite mainstream and talked about. It seems to be everywhere at the moment. Patrick has played around with it but knows plenty of people are using it professionally and casually. It's not really a secret that people are using it to help with work.

Several of us have used it to look up some code but already has lots of concerns around the accuracy of the code - Huw even used it to write a [data ethics writeup](https://dataethicsclub.com/write_ups/2022/14-12-22_writeup.html) and detailed his experiences using it [here](https://dataethicsclub.com/write_ups/2022/ChatGPTStoleMyJob.html). Ola has played around a little bit, and it is a bit basic, dryly communicating the message rather than creating a beautiful argument. It's got a very strict structure. 

Many of us have read about undergrads writing essay based questions using it and heard about markers being told to stay on the look out for signs of it being used. Has ChatGPT made traditional homework assessments all but impossible? Does this tool require us to reform the way we assess in education? If so, what does that say about the way we assess in education? Using ChatGPT to cheat writing essays should tell us that essays aren't the best way to assess knowledge/creativity. Why is it surprising that ChatGPT can do as well as a last-minute essay written overnight before the deadline? It's the same process of throwing whatever rubbish on the page and hoping for the best!

Noshin hasn't used it yet, but has noticed school application systems are upping their exam pass thresholds to try and get around this. Schools don't appear to have a way of combating this yet. If you ask ChatGPT where it got its data from, it doesn't always know! This doesn't mean it won't get flagged for plagarism.

We have seen examples of ChatGPT being used to write CVs and cover letters which are better received because they are being read by language models. What happens if you get ChatGPT to write a cv based on your qualifications? Does it write something similar to what you wrote?

Zoe has used ChatGPT in work. "write a blog about NHS R community" - results were very boring! 11-year-old son of Zoë's colleague has used it to make up stories. Could this capability be used maliciously to write conspiracies? Almost certainly. Perhaps the scariest part are the applications outside academia. People think it's a search engine (as Bing have) and just believing the answers it gives. Mathoverflow already had a deluge of nonsense answers written by Chat-GPT. How would the pandemic have gone if ChatGPT was around and mainstream earlier? Would there have been even more conspiracies? 

Some of us have asked ChatGPT about itself. It has lied about it's training data, was said it was up to July 2021 but if you ask who is the ceo of Twitter then it knows but that occurred later. When tackled, it lied just like humans are capable. Are we anthroporphising though by saying lying? That's not what's happening, something else is happening but we don't have terminology for it.

You can even check if you are in the image database https://haveibeentrained.com/

A language model that was constantly being trained by a variety of inputs could easily be skewed into regurgitating conspiracies and false statements. There's not a clear consideration of truth in the algorithm as the ground-truth is sometimes up for debate, as is the case in real life. There could even be psychological impacts on people who have to review the content being generated. Miranda did research of GPT3 showing bias against different cities in the UK e.g. "A Bristolian walks into a courtroom". 

"It's just like that guy at the party who thinks he knows all the stuff" as one our members put it. Or perhaps a STEM major weighing in on social issues they know nothing about with the same confidence they talk about their own area of expertise.

One of the benefits - people care about this suddenly a lot. It's been getting closer to this for ages, but very few people are actually concerned.
Hopeful - Microsoft have bought it. If it's integrated with office tools we'll all be using it, and Microsoft will need ethical accountability.
We don't rely on what we think is ethical, we rely on private companies and what they think is ethical. Eg. America has no GDPR - lots of data security breaches.

We had concerns around data storage - what info is it holding about you and how safely is this secured? Technically a lot of what researchers write belongs to the university anyway, but in the US people care more about monetary compensation for their data, whereas in Europe the focus is on credit and privacy of data. Companies can collect data initially for one reason, but then if it's picked up by other services (chatGPT, Facebook etc) they don't realise it's then being used for completely different purposes.

### Should ChatGPT be allowed to be listed as an author on publications?

What accountability do we expect from an author that we can't get from ChatGPT? Supervisors often get names on papers even if they do nothing. Often undergrads get left off authorship lists, we shouldn't then include ChatGPT if we're neglecting human contributors. A point Robin made was that it’s weird how people seem almost more likely to allow ChatGTP as a Co-author than allowing an undergrad research assistant who has actually done a lot of the lab work. They often are relegated to acknowledgements at best. What does that says about the position of research students in the science hierarchy? 

Academics aren't great at calling each other out for poor behaviour. Some journals are getting better at properly attributing credit to authors. Language models are clearly not authors! Authorship on papers sometimes doesn't reflect what people might expect. (who did the actual work? if the journal doesn't use proper credit attribution then how can a random reader tell?) The lines blur between contributing as an author when you've just checked something over as opposed to writing hundreds of words.

Thanking your mum and dad in acknowledgments doesn't mean mum and dad should be co-authors. If someone who hardly did any of the work/writing can be an author then why not ChatGPT though? The problem is the academic culture surrounding attribution of credit. This also ties into deeper problem with the superficial metrics of 'success' in academia.

Human authors don't always also have accountability; eg. papers with 3000 authors, so where we draw that distinction? Viv mentioned a paper which has ChatGPT as the lead author. What does that say about the contributions of the co-authors? Huw is yet to see an example of ChatGPT being a paper co-author that wasn't laziness or novelty. 

It all comes down to how you define an author - are you copying text from ChatGPT word for word or are you citing it? Can it be a reference or acknowledgement instead? Citations are maybe next best place but do Language Models need their own form of citeable identifier? ChatGPT can't be legally responsible like an author can. It can get the right grammar and sentence structure but it can't know if the information is correct. A 'senior prof' author on a paper who gets put on without doing anything is different because the human should be responsible for any inaccuracies!

People should be transparent if they've used it but should we give them credit for writing with it? It's just taking other people's content and re-working it. It's not original thought but people do that all the time? Could you develop some kind of explainability system? How hard are these systems to explain? We expect the work that is produced to be credited and cited appropriately.  If we can't give credit where its due then maybe we shouldn't use ChatGPT. If you were to plagarise a piece of writing knowingly then that's clearly bad. Why should it be any different if you get a black box system to plagarise many sources in a way you don't understand?

### How should we use language models like ChatGPT going forward?

There are some fun, novel things you can do with ChatGPT: Huw used it to write a haiku about his mum's violin playing waking the dog up. Creating and commenting on coding is an excellent use of it is also very useful and a good application of it. There are issues with relying on being able to access ChatGPT to do your job in the case of outages. Additionally, those of us who had tried using ChatGPT to help with code felt that it inhibited our ability to learn that code. This is in contrast to the standard method of trial and error. 

But is ChatGPT just used for novelty or is its main value entertainment? What's the breakdown? When the creators made it, what use-cases did they have in mind?

At the end of the day, we are training data for this model but to what end? If you're not paying for something then you're the product.

Individually it may not matter if one output is true or not, but cumulatively this could get really bad. As people use these tools more and more, they will check the accuracy less and less. 
Every problem is self-reinforcing, as ChatGPT learns from its own outputs. Will people ever have time to check all the outputs?
    
ChatGPT talks very confidently even when it's completely wrong. But people are also really bad at being confidently incorrect, especially in certain fields, however we are still accountable for mistakes or misinformation!

We need to fight back by being more creative, more accountable, more open about our flaws etc. We can't uninvent ChatGPT, but we can force it to be better by being better ourselves (i.e being more human by acknowleging mistakes and being uncertain!)

There's *this* [flowchart](https://twitter.com/kareem_carr/status/1619743014813052930/photo/1) for good use. It presents a nice premise, but the trouble is, who is actually using it this way?

Maybe ChatGPT will influence how we write and speak? Is it about time for the death of the essay? How is it updated?

What about Google's model? Maybe they found a way to link to the sources or maybe they want to keep you on their site through summaries of their results


Might need to completely reform publishing as well as education?

Zoë can't be bothered to sign up and doesn't like the idea of paying which might come in. Is Huw not using it anymore because he feels like he's cheating?

A payment model might be what ends up happening here in the same way people pay for grammarly.

"People trained a language model to sound like a corporate manager and concluded that the model is sentient rather than the correct answer whichis that corporate managers aren't."

--- 

## Attendees

__Name, Role, Affiliation, Where to find you, Emoji to describe your day__
- Natalie Zelenka, Data Scientist, University of Bristol, [NatalieZelenka](https://github.com/NatalieZelenka/), [@NatZelenka](https://twitter.com/NatZelenka) 
- Nina Di Cara, Research Associate, University of Bristol, [ninadicara](https://github.com/ninadicara/), [@ninadicara](https://twitter.com/ninadicara)
- Huw Day, PhDoer, University of Bristol, [@disco_huw](https://twitter.com/disco_huw)
- Euan Bennet, Lecturer, University of Glasgow, [@DrEuanBennet](https://twitter.com/DrEuanBennet)
- Noshin Mohamed, Children's Quality Assurance 
- Vanessa Hanschke, PhD Interactive AI, University of Bristol
- Melanie Stefan, Computational Biologist, Medical School Berlin 
- Zoë Turner, Senior Data Scientist, Strategy Unit/NHSR Community, @Letxuga007@fosstodon.org
- Laura Williams, Senior Data Scientist, Ministry of Justice
- Ola Michalec, researcher at Bristol Univ 2
- Robin Dasler, research data product manager, [daslerr](https://github.com/daslerr/)
- Amy Joint, Content Acquisition Manager, F1000 - @AmyJointSci

