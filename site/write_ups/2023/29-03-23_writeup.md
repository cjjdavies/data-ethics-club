# Data Ethics Club: [The Tech We Won't Build](https://2022.internethealthreport.org/episodes/the-tech-we-wont-build/)
<!--Please don't edit the info panel below-->

<!--

```{admonition} What's this? 
This is summary of Wednesday 28th July's Data Ethics Club discussion, where we spoke and wrote about the New Republic article [The Rise of Private Spies](https://newrepublic.com/article/161913/we-are-bellingcat-spooked-private-investigators) by Charlie Savage.
The summary was written by Jessica Woodgate and Huw Day, who tried to synthesise everyone's contributions to this document and the discussion. "We" = "someone at Data Ethics Club". 
Nina Di Cara and Natalie Thurlby helped with the final edit.
```

-->




## Discussion

### Discussion

## Discussion

### Discussion

##### Q1 What mechanisms or options exist to you in your context (work or otherwise) to resist the development of technologies you morally disagree with?

We loved the idea of individuals calling out poor or unethical research mentioned in the podcast (where Google employees called out the US defence contract Project Maven for [spying on people]( https://www.wired.com/story/the-line-between-big-tech-and-defense-work/)); employees should have power to hold their employers accountable. To counter misinformation in academia, [Elisabeth Bik]( https://twitter.com/MicrobiomDigest?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) calls out manipulated images and has had hundreds of papers retracted. [Hotlines]( Whistleblowing: list of prescribed people and bodies - GOV.UK (www.gov.uk)) exist for reporting unethical behaviour, however this can be hard to do alone. Forming groups might help to support one another and create grassroots power. Talking to the people you are working with, as was done in the Project Maven example, might be a more effective way of promoting change, rather than just leaving the project and allowing it to go ahead anyway – just without your input.


*What issues arise with the ability to morally object in the workplace?*


However, raising the alarm bell might be more difficult if you are working in a country you haven’t worked in before. It can be less clear what the norms are, and how they should be appropriately challenged. A lot of researchers move internationally for their work, so this might be something that crops up a lot in these communities.

In addition to knowing the norms in place, the size of an organisation might impact your ability to challenge something you morally disagree with. In large organisations, it is much easier to get lost amongst the weeds. Even if you raise your head above the parapet, it could be soon forgotten, increasing the importance of having colleague support.

Difficulties with garnering the support of your colleagues arise with the growth of remote working. Often, these sorts of conversations are important to feel out informally first, to garner perspective and general thoughts. Opportunities for informal interactions with colleagues are heavily reduced in remote work scenarios.


*Different levels of authority may have different motivations; how does this affect our freedom to choose ethical paths?*


On a personal level, PhDoers might have autonomy to decide not to go down certain paths, but this could be limited by funding. Sometimes there are incentives behind funding bodies that limit the freedom of choice for the direction you take, and it isn’t always possible to know what those incentives really are, or the reasoning behind them. The motivation of other bodies supporting, and to some extent constraining, your work might be different to yours and this might be hidden from you.

Diversion of motivation from authoritative bodies and choice constraint continues in more focussed positions like postdoc projects and industry jobs. It can be pretty cut and dry in these domains: you either do the goals you’re given, or you lose your job! There is also the sunk cost fallacy in defence projects, removing the excuse of “this doesn’t work”. For example, the SA-80 British made assault rifle which is [famously rubbish](https://nationalinterest.org/blog/buzz/introducing-sa80-worst-military-rifle-ever-44987) , and yet the military refused to abandon it.

Continuing support for projects when they do not make financial or ethical sense brought us to the different forces that exist behind research, as in defence there are not just financial but also political motivations. For example, in the SA-80 case, the military don’t want to use an American rifle so will carry on using ineffective technology.


*What are our own motivations for our work?*


When choosing a career, it is important to think carefully about the risks and benefits of the kinds of positions and companies you undertake. For some, there might be the option of a very highly paid job. However, there might be ethical compromises that come with the paycheck. Knowing that the wage might be hard to leave, should the job be avoided in the first place?

As a counter to having to choose between being economically comfortable and having an ethical career, perhaps a guaranteed minimum income could ensure that people are less likely to do horrible things or unethical jobs. Some of us had already come upon situations where we were presented with relatively harmless roles, but for organisations whose motivations we are not comfortable with, and decided against taking the position.
We must think about the kinds of compromises we are making: our ethics; our job security; our financial security; our creative freedom?


##### Q2 A lot of research lives in a 'grey' ethical area - but are there clear thresholds for the Tech We Won't Build?

*How could our work be misapplied?*


We wondered if anything is really ethically neutral, especially in the domain of research. It is conceivable that something which seems unimportant now could have unintended side effects when it is put into different contexts. You can carry out your research, focussing on things you perceive as more “ethical” – whatever that means to you – but from an academic perspective, when you put something out there, it’s out.  Technologies we develop could have unforeseen long term effects, such as Huw’s Ph.D., which now has unclear implications, but what if 50 years from now someone uses it to make a bioweapon? Will Huw finally do something useful? 

Network science is another example domain which might seem initially unsuspicious, yet there is a lot of defence funding that goes into the area, with military applications such as looking at social networks to detect terror cells. Similarly, in the education sector the integration of technology could result in biased and harmful prophecies for children. Teachers can have preconceptions of students from certain backgrounds, which may lead to models limiting those students’ access to resources by predicting what they might (not) achieve.

Unfortunately, in domains like maths and physics, it is common for abstract models to be developed and then later down the line applied in harmful ways. If someone wants to build it, a lot of the time it is quite easy to do. At the very least, we thought you can avoid being the final step (i.e. the harmful application).


*When do we decide when not to build something?*


Considering the risk of unforeseen harms, at what point do you decide not to build technology? How far away from harm do you have to be down the causal chain to have responsibility for the consequences of your work? Is it when there are very clear implications that Huw’s work could be used for genocide, or something else awful, like a boring PhD thesis? How much harm will he cause his reviewers? Should he be stopped now?

It is important to distinguish the point at which you halt development, from where mitigation techniques should be brought in. Maybe someone could develop some sort of signposting to help this... Something like [Data Hazards](https://datahazards.com/) labels? Just a thought.

To understand where these thresholds are, the podcast talks about projects in the domain of high risk, high consequence. If there is a high probability that things could go wrong, and if things go wrong they go really wrong, maybe you should stop. [An article we read in a previous reading group](https://journals.sagepub.com/doi/10.1177/20539517221146122) provided a quite simple but useful heuristic for machine learning.  The paper suggests that ML should be used for well-defined, ‘learnable’, and low stakes problems, with relatively small ranges of subjectivity. This might help when we are trying to think about what could be done, or go wrong with, the models we build.


*Removing responsibility through depersonalisation?*


There is often the reasoning in tech that “we’re only making a tool” and thus aren’t accountable for misapplications of our work. However, it is difficult to know where this line should be drawn. The scope of some tools might be quite generic with wide ranging application domains, some of which could be harmful. For example, license plate recognition could be based off of generic text/image recognition. If you follow this reasoning too far, it can lead to unintuitive outcomes. Should we blame the inventors of the wheel for tanks? How many degrees of separation do you need to claim the technology that you developed is nothing to do with harm that is caused? This highlights the ambiguity of ethical reasoning.
To define the tipping point of accountability, some of us had the intuition that if there’s a harm you could reasonably foresee then you hold some sort of responsibility to make efforts towards mitigation. However, if it is completely out of the realm of what you could conceive then you are off the hook. We also thought that it might depend on what you could do personally to mitigate the risk of bad things happening; what is in your personal control.

Going above personal responsibility, ethical frameworks can help to support ethical development on an institutional level. For example, the concept on research *on* people vs research *with* people, which is key in early years education. We should think reflexively, including the people for whom the technology might be used into the design and development process, so that their needs remain front and centre. If they can’t be directly involved, we should at least make efforts to consider their perspectives.



##### Q3 How do we build accountability into tech development so that we can challenge decisions and directions we disagree with?

We really liked the quote "can we build this? Yes. Should we build it? Depends on who you ask." The example of [robodebt] (https://www.theguardian.com/australia-news/2023/mar/11/robodebt-five-years-of-lies-mistakes-and-failures-that-caused-a-18bn-scandal) was brought up – a governmental scheme which led to hundreds of thousands of people being attributed unlawful debt with the intention of “saving money”. It demonstrates how little accountability there is for people who make terrible decisions as leaders. However, perhaps we underestimate how little people understand about the workings of AI and data science. We can see how something might appear a great solution, if you don’t really understand what it means.
We were quite keen to see further discussion on the military usage of tech. There is an old-timey view that it’s all bad, and whilst there is ground for that, it is still worth talking about. Some of us have had conversations with people who work in defence and are interested in ethics. It is important to try not to get into an “us vs them” mentality when considering such topics; we are talking about real people and should try to understand their perspectives.

At the very least, you need a clear chain of accountability in defence. Pursuing the requirement for clear accountability in the military could lend a hand to other domains seeking better accountability practices. The domain of defence is interesting to consider, as it has a very different focus, for example whilst the private sector is heavily profit-oriented, the military has other motivations such as politics. What emerges from this difference? Does it affect motivations for ethics, or accountability?

--- 

## Attendees

__Name, Role, Affiliation, Where to find you, Emoji to describe your day__
- Natalie Zelenka, Data Scientist, University of Bristol, [NatalieZelenka](https://github.com/NatalieZelenka/), [@NatZelenka](https://twitter.com/NatZelenka) 
- Nina Di Cara, Research Associate, University of Bristol, [ninadicara](https://github.com/ninadicara/), [@ninadicara](https://twitter.com/ninadicara)
- Huw Day, PhDoer, University of Bristol, [@disco_huw](https://twitter.com/disco_huw)
- Ola Michalec, University of Bristol - social scientist in comp science school. [@Ola_Michalec](https://twitter.com/Ola_Michalec)
- Amy Joint, Content Acquisition Manager, F1000, [@AmyJointSci] (https://twitter.com/amyjointsci)
- Euan Bennet, Lecturer, University of Glasgow, [@DrEuanBennet](https://twitter.com/DrEuanBennet)
- Robin Dasler, Data Product Manager, [daslerr](https://github.com/daslerr/)
- Jessica Woodgate, PhD Student, University of Bristol [jess-mw](https://github.com/jess-mw)


<!--
Please ignore these grey bits of text at the bottom of our HackMD documents. These help us to quickly format the write-ups for the website.
-->

<!--
How to put a writeup on the website:
1. Go to the GitHub Site (https://github.com/very-good-science/data-ethics-club) and navigate to the `site>contents>write_ups` directory, and go inside the directory for the current year.
2. Above the file browser, on the right, choose the option `Add file > Create New File`, name the file `DD-MM-YY_writeup.md` (date of the Data Ethics Club meeting). 
3. Copy in the contents of the write up (click next to the `1` in GH which is indicating the first line of the file). Add the admonition code (in the comment below this one) just below the title, but before the rest of the write up and change the text accordingly. Put the attendees at the bottom of the write up.
4. IMPORTANT: to save, scroll down the the bottom and choose the "Create a new branch for this commit and start a pull request" option (this is not the default). Name the new branch something relating to the write up, e.g. "PrivateSpies", then click "Propose New File"
5. You will be taken automatically to create a pull request. Write in any things that you'd like any particular feedback on, in the comments. Add Natalie or Nina as reviewers.
6. You then need to add the markdown document to the Table of Contents in the navigation bar by editing the `writeup.md` file ON THE BRANCH YOU JUST MADE. You can go to the branch you just made from the Pull Request page, as the Pull Request will say something like "NatalieTurlby wants to merge 1 commit into `main` from `YOUR_BRANCH_NAME`" (click YOUR_BRANCH_NAME). Alternatively, you can get to it by going to the homepage of the GH repository, and choosing from the dropdown menu on the left. Once you are on your branch, navigate to `site>contents>write_ups>write-ups.md`, choose "edit this file" (little pencil), add your new file's location, and commit changes with the "Commit directly to the YOUR_BRANCH_NAME branch".
7. All done! Comment here to let me know how these instructions worked out: https://github.com/very-good-science/data-ethics-club/issues/86
-->

